{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load dictionaries and batches\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class W2V:\n",
    "    def __init__(self,\n",
    "                 dictionary_fname='../data/protein/dictionaries_1.json',\n",
    "                 csv_fname='../data/protein/test2.csv',\n",
    "                 save_path='test.ckpt',\n",
    "                 batch_size=128,\n",
    "                 embedding_size=100,\n",
    "                 num_sampled=5,\n",
    "                 seed=0):\n",
    "        with open(dictionary_fname) as f_in:\n",
    "            self.id2w = json.loads(f_in.readline())\n",
    "            self.w2id = json.loads(f_in.readline())\n",
    "            self.word_count = json.loads(f_in.readline())\n",
    "\n",
    "        # this uses lots of memory\n",
    "        self.csv = np.genfromtxt(csv_fname, delimiter=\",\", dtype=np.int32)\n",
    "\n",
    "        # this is not a hack, but rather due to the fact that integers can't be keys in JSON\n",
    "        # today I learned\n",
    "        self.id2w = dict((int(k),v) for k,v in zip(self.id2w.keys(), self.id2w.values()))\n",
    "        self.batch_size = batch_size\n",
    "        self.every_2000_losses = []\n",
    "        self.save_path = save_path\n",
    "        vocabulary_size = len(self.w2id.keys())\n",
    "        self._data_index = 0\n",
    "\n",
    "        # build the computation graph\n",
    "        self.session = tf.Session()\n",
    "        with self.session.graph.as_default():\n",
    "            tf.set_random_seed(seed)\n",
    "\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size,1])\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "                # This is a matrix that holds the embeddings, random initialization\n",
    "                embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "                # This is a view into that matrix for the inputs in the batch\n",
    "                embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n",
    "\n",
    "                # Weights and Biases for Log Reg on embeddings\n",
    "                nce_weights = tf.Variable(\n",
    "                    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                        stddev=1.0 / math.sqrt(embedding_size))\n",
    "                )\n",
    "                nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "            # Compute the average NCE loss for the batch. Automatically\n",
    "            # does negative sampling too.\n",
    "            self.loss = tf.reduce_mean(\n",
    "              tf.nn.nce_loss(nce_weights, nce_biases, embed, self.train_labels,\n",
    "                             num_sampled, vocabulary_size))\n",
    "\n",
    "            # Construct the SGD optimizer (minimizer) using a learning rate of 1.0.\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(self.loss)\n",
    "\n",
    "            # Compute the co            tf.initialize_all_variables().run()sine similarity between minibatch examples and all embeddings.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "            self.normalized_embeddings = embeddings / norm\n",
    "            \n",
    "\n",
    "        tf.initialize_all_variables().run(session=self.session)\n",
    "            \n",
    "    def gen_batch(self):\n",
    "        if (self._data_index + self.batch_size) > self.csv.shape[0]:\n",
    "            self._data_index = 0\n",
    "\n",
    "        batch = self.csv[self._data_index:self._data_index+self.batch_size,0]\n",
    "        labels = self.csv[self._data_index:self._data_index+self.batch_size,1,None]\n",
    "\n",
    "        self._data_index = self._data_index + self.batch_size\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def train(self, num_batches):\n",
    "        average_loss = 0\n",
    "        for step in range(num_batches):\n",
    "            batch_inputs, batch_labels = self.gen_batch()\n",
    "            feed_dict = {self.train_inputs : batch_inputs,\n",
    "                         self.train_labels : batch_labels}\n",
    "            _, loss_val = self.session.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            self.every_2000_losses.append(average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    def gen_embedding(self):\n",
    "        return self.normalized_embeddings.eval(session=self.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.12862423e-01,  -4.83706035e-02,   4.85960618e-02, ...,\n",
       "          7.32316077e-02,   1.08804017e-01,  -1.88616887e-01],\n",
       "       [ -9.55439806e-02,  -4.81215082e-02,  -1.66858345e-01, ...,\n",
       "          5.56581877e-02,  -4.32461537e-02,  -1.09289713e-01],\n",
       "       [ -1.10054888e-01,   6.62492439e-02,  -1.89714476e-01, ...,\n",
       "          3.04480847e-02,   1.52271971e-01,  -7.61605278e-02],\n",
       "       ..., \n",
       "       [ -3.87193188e-02,   2.09836736e-02,   1.67491392e-03, ...,\n",
       "          2.09090840e-02,  -8.79947692e-02,  -6.67165369e-02],\n",
       "       [  1.19331688e-01,  -1.29233167e-01,   1.58526748e-01, ...,\n",
       "          1.41413420e-01,   1.20201983e-01,  -1.61211208e-01],\n",
       "       [ -1.59572631e-01,  -2.16572024e-02,  -7.15490532e-05, ...,\n",
       "          7.69217610e-02,  -8.05691406e-02,   1.48426533e-01]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = W2V(dictionary_fname='../data/protein/dictionaries_1.json',\n",
    "          csv_fname='../data/protein/test2.csv',\n",
    "          save_path='test.ckpt',\n",
    "          batch_size=8,\n",
    "          embedding_size=100,\n",
    "          num_sampled=5)\n",
    "\n",
    "w2v.train(100)\n",
    "w2v.gen_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
